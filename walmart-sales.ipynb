{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walmart Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from darts import TimeSeries\n",
    "from utils import TimeseriesExperiment, Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weekly_Sales</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-02-07</th>\n",
       "      <td>24924.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-14</th>\n",
       "      <td>46039.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-21</th>\n",
       "      <td>41595.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-02-28</th>\n",
       "      <td>19403.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-03-07</th>\n",
       "      <td>21827.90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Weekly_Sales\n",
       "Date                    \n",
       "2010-02-07      24924.50\n",
       "2010-02-14      46039.49\n",
       "2010-02-21      41595.55\n",
       "2010-02-28      19403.54\n",
       "2010-03-07      21827.90"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_store = pd.read_csv('data/walmart-sales/stores.csv')\n",
    "\n",
    "df_train = pd.read_csv('data/walmart-sales/train.csv')\n",
    "\n",
    "df_features = pd.read_csv('data/walmart-sales/features.csv')\n",
    "\n",
    "# Get data from one store\n",
    "df = df_train[df_train['Store'] == 1]\n",
    "df = df[df['Dept'] == 1]\n",
    "# TODO: Use is holiday parameter\n",
    "df = df.drop(['Store', 'Dept', 'IsHoliday'], axis=1)\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.set_index('Date').resample('W').mean()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.dataprocessing.transformers.scaler import Scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def get_series_from_dataframe(dataframe):\n",
    "    s = TimeSeries.from_dataframe(dataframe, freq='W', fill_missing_dates=True)\n",
    "    # s = s.slice_n_points_before(s.end_time(), TRAIN_DAYS*24*60)\n",
    "    # s = s.resample('1week', method='pad')\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    transformer = Scaler(scaler)\n",
    "\n",
    "    result = transformer.fit_transform(s)\n",
    "\n",
    "    return result\n",
    "\n",
    "series = get_series_from_dataframe(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Date'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "series['Weekly_Sales'].plot()\n",
    "# (series['IsHoliday'] * series['Weekly_Sales'].max()).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(series, 'walmart-sales')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mape': 737.6608937158287,\n",
       " 'mse': 2.0007609387042296,\n",
       " 'rmse': 1.4144825692472247,\n",
       " 'mae': 0.8765371194128259,\n",
       " 'model': 'NaiveSeasonal',\n",
       " 'forecast_horizon': 3,\n",
       " 'dataset': 'walmart-sales',\n",
       " 'experiment_time': 1715278069.307275,\n",
       " 'parameters': OrderedDict([('K', 1)])}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from darts.models import NaiveSeasonal\n",
    "\n",
    "model = NaiveSeasonal(K=1)\n",
    "\n",
    "experiment1 = TimeseriesExperiment(model, dataset, retrain=True)\n",
    "\n",
    "experiment1.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mape': 910.9735583727415,\n",
       " 'mse': 2.0904467779306355,\n",
       " 'rmse': 1.4458377426013735,\n",
       " 'mae': 0.9286568443229403,\n",
       " 'model': 'ARIMA',\n",
       " 'forecast_horizon': 3,\n",
       " 'dataset': 'walmart-sales',\n",
       " 'experiment_time': 1715278529.2951732,\n",
       " 'parameters': OrderedDict([('p', 1),\n",
       "              ('d', 1),\n",
       "              ('q', 0),\n",
       "              ('seasonal_order', (0, 0, 0, 0)),\n",
       "              ('trend', None),\n",
       "              ('random_state', None),\n",
       "              ('add_encoders', None)])}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# arima\n",
    "from darts.models import ARIMA\n",
    "\n",
    "model = ARIMA()\n",
    "\n",
    "experiment2 = TimeseriesExperiment(model, dataset, {\n",
    "    'p': [1, 3, 5],\n",
    "    'd': [1],\n",
    "    'q': [0]\n",
    "}, retrain=True)\n",
    "\n",
    "experiment2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for best parameters {'lags': [1], 'max_depth': [3, 5, 7], 'n_estimators': [50, 100], 'output_chunk_length': [1]}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "713063e0361b4cad93df893728214d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`enable_optimization=True` is ignored because `retrain` is not `False` or `0`.To hide this warning, set `show_warnings=False` or `enable_optimization=False`.\n",
      "`enable_optimization=True` is ignored because `forecast_horizon > model.output_chunk_length`.To hide this warning, set `show_warnings=False` or `enable_optimization=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb1e97020cd49cf96b94469d80d75ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`enable_optimization=True` is ignored because `retrain` is not `False` or `0`.To hide this warning, set `show_warnings=False` or `enable_optimization=False`.\n",
      "`enable_optimization=True` is ignored because `forecast_horizon > model.output_chunk_length`.To hide this warning, set `show_warnings=False` or `enable_optimization=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae81f97663841f38ad6a7ad32f63e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`enable_optimization=True` is ignored because `retrain` is not `False` or `0`.To hide this warning, set `show_warnings=False` or `enable_optimization=False`.\n",
      "`enable_optimization=True` is ignored because `forecast_horizon > model.output_chunk_length`.To hide this warning, set `show_warnings=False` or `enable_optimization=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6877ea932f8647faa709eb55ee4ebe95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`enable_optimization=True` is ignored because `retrain` is not `False` or `0`.To hide this warning, set `show_warnings=False` or `enable_optimization=False`.\n",
      "`enable_optimization=True` is ignored because `forecast_horizon > model.output_chunk_length`.To hide this warning, set `show_warnings=False` or `enable_optimization=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "778e017b249e4c719b07ed56a1144f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`enable_optimization=True` is ignored because `retrain` is not `False` or `0`.To hide this warning, set `show_warnings=False` or `enable_optimization=False`.\n",
      "`enable_optimization=True` is ignored because `forecast_horizon > model.output_chunk_length`.To hide this warning, set `show_warnings=False` or `enable_optimization=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e7ee80d80246b096b9836002735708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`enable_optimization=True` is ignored because `retrain` is not `False` or `0`.To hide this warning, set `show_warnings=False` or `enable_optimization=False`.\n",
      "`enable_optimization=True` is ignored because `forecast_horizon > model.output_chunk_length`.To hide this warning, set `show_warnings=False` or `enable_optimization=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2371c29805a4a10b8258c49f113f4c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/109 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`enable_optimization=True` is ignored because `forecast_horizon > model.output_chunk_length`.To hide this warning, set `show_warnings=False` or `enable_optimization=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'lags': 1, 'max_depth': 3, 'n_estimators': 50, 'output_chunk_length': 1} Metric: 482.0236097750885\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mape': 548.4061723689607,\n",
       " 'mse': 0.7510370082466515,\n",
       " 'rmse': 0.8666239139595974,\n",
       " 'mae': 0.5168625236470514,\n",
       " 'model': 'XGBModel',\n",
       " 'forecast_horizon': 3,\n",
       " 'dataset': 'walmart-sales',\n",
       " 'experiment_time': 1715278088.2571268,\n",
       " 'parameters': OrderedDict([('lags', 1),\n",
       "              ('lags_past_covariates', None),\n",
       "              ('lags_future_covariates', None),\n",
       "              ('output_chunk_length', 1),\n",
       "              ('output_chunk_shift', 0),\n",
       "              ('add_encoders', None),\n",
       "              ('likelihood', None),\n",
       "              ('quantiles', None),\n",
       "              ('random_state', None),\n",
       "              ('multi_models', True),\n",
       "              ('use_static_covariates', True),\n",
       "              ('max_depth', 3),\n",
       "              ('n_estimators', 50)])}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from darts.models import XGBModel\n",
    "\n",
    "model = XGBModel(\n",
    "    lags=1,\n",
    ")\n",
    "\n",
    "experiment3 = TimeseriesExperiment(model, dataset, {\n",
    "    'lags': [1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'n_estimators': [50, 100],\n",
    "    'output_chunk_length': [1]\n",
    "})\n",
    "\n",
    "experiment3.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20:26:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:30 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:30 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:31 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:32 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:33 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:33 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:34 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:34 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:35 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:36 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:36 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:37 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:37 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:38 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] start processing\n",
      "20:26:39 - cmdstanpy - INFO - Chain [1] done processing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mape': 945.70550394856,\n",
       " 'mse': 1.492013362477232,\n",
       " 'rmse': 1.2214799885701084,\n",
       " 'mae': 0.801439171553654,\n",
       " 'model': 'Prophet',\n",
       " 'forecast_horizon': 3,\n",
       " 'dataset': 'walmart-sales',\n",
       " 'experiment_time': 1715279199.9038038,\n",
       " 'parameters': OrderedDict([('add_seasonalities', None),\n",
       "              ('country_holidays', None),\n",
       "              ('suppress_stdout_stderror', True),\n",
       "              ('add_encoders', None),\n",
       "              ('cap', None),\n",
       "              ('floor', None)])}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from darts.models import Prophet\n",
    "\n",
    "model_prophet = Prophet()\n",
    "\n",
    "experiment_prophet = TimeseriesExperiment(model_prophet, dataset, retrain=True)\n",
    "\n",
    "experiment_prophet.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b403ad915e249328a102e98194c3496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "`enable_optimization=True` is ignored because `retrain` is not `False` or `0`.To hide this warning, set `show_warnings=False` or `enable_optimization=False`.\n",
      "`enable_optimization=True` is ignored because `forecast_horizon > model.output_chunk_length`.To hide this warning, set `show_warnings=False` or `enable_optimization=False`.\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No parameters to search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d66cd83da4146d8b288bfee3e5c79d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c06814dba0f24cec8b56337ef3d371ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e3efe2772d947f2ac5e701d62564b46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71a5f3fbef254d9996abb76c9b95a896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b91e81607d34fb399e8b2265733f098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076a793da6424859bacd8f000281a2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b9e13c1fca54a098b142b13ef946188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c52c841bf594404ab173464d209acfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fac65f828964834a4772e7ad8c4cbb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0c829446894231a10ec56b63c7a6e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a96f977b03a74a62bb98895e086aab23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0565284801e84533b22abfe06bd2f74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeabb32a526749ba851bdd23b0b9c541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd2238e261824ae39a2a29ca4b2ec9fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a84785fbbe17464999ca22dea763ef8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c5659cd2024fd1ba48762358ffad0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85c2f29b69640d7a14b1f1c397baea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1eb31758804bd3bff5b35c391ccb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0852de65ed404340b4b53605da78616b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af2fc01fe664acea184daea225c2353",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "957000c1a4f547ffa8cd5739e16a9c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6292d12fa5644cdc9767ceeab39f2285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2a047a51274271a25624b0798ce10c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19de844b0c334ba1bf583a279d41e2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1213faec24e94975bac9260f74143abe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49e236848324c799317dac8a1472027",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67242c44065b45e09af4bf7b45e7ca3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d1b43d3064b4bcc9a54119fbc6c5f6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423e9592eaf54eab8693c5f590216cbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e9e1dc16884dfba9cbe865e65eb603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a827f8e68b424dc5b6e3782fb8922179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffc4c329f3264724a4fd6e47c34082f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ba9ca765cf459ea3f993d7644c754a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e068d625644293900ec101bb7bcac9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d74ffe27925480ab3773c78990e5a4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76ed58ec9290446eb9b7a3b0d1c308c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8705385fb3a4c53802c76819414e8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f074c197502548a481d91fc679c30883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89abe6a44f04f43916234fcd26a3108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca3bb979e9654558b7e41b83a2730e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a7fc776001c45b3b3227c9c94634df1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "049e63aa3cc54cbcaabe9fbaf1c45a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c622bea77dec4b3b8ec700cdaafc5428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9920112a2e0946db97b3bf44623c92d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1617c8a07f09483ab043891a18112713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cc25d832eac4e3586ab9cc18fbfc213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9aae86791a49fb85d1a9083b0daa0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (3) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8f13b532dc42eeb2e64d1ddc755854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcbee29134034c6cae8c0bf724f0f02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143f72b8450a424ea337dc273dbd4fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c284fe37e2824fcfa1c2bbc5dab1f860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d338d8f1575d4bed8e3f06c64fb7e417",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f386d64e0cdc455e8e5811dcbe0ee1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62137c62c39c4aa79de2b57a84729e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d8bc4e0fae4c0083e15825e059a858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2f2ff7018840829a3b55bf9e5e963b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7580db74827c429f879a8327880582b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46021bc394de468780f26dca3c68188e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32ab56ba7cbd4f0485dc5854fdfaf211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eede49de0b4c42c49af2c68fe30e0db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abd02e14b3c64c7ab35418054725a5c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7d097f66de4f9b94847d5d1084250b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d035efbb459a4e158aaf5050b9d546dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (4) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7d72a62f1e24d8f908bb590d7748436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc4d08119c59409bb1c74256a42930e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08ca6608d024f13a418e9448d6869f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f0262387c84f2fa7ff4daf7562b122",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "900f3e9865274a9aac02bde23ef77dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "847ae194e36b4d3eb49d8b19979d0011",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1cb5c90336a4c3cb52573f89831acd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146bf78d8297421aba47f72c6faab8bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0467078f44e04a07a1e19e1f060b0676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7cbe0c4c3346fcba72ea69cb3fa5d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3fd499233c2452e905f39c703c8b2db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f02ef5ff0c4c10b5315d9c5bf2c816",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6707b0eae25146a4802c5077c2d596ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e7d9b09a55b4952a0447d33f4520d22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa12a5472fae4be0951d6e6a5e8da1ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316c7713c830403aa0893955941f1997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (5) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296d863e585e44328987ab72160bfb9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f604606fd71f43bc9ede2feb12a48509",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b4017b3cec246a9a0d2bf78864cec02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b3fc92a7444fa0a6c4d5cb11ac28d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e70714c8e9f0423c9109fe97ace73a4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92998d7703744e33aa0b667771d1676e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75afe7df25ca49a4971f8b47335b29c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e58cd7b6494e67bebb907b34f3be59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504bdd5b9cd345d6940ce77dfdf8872a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d69bb95bf5db4bc5868571fa5cd957c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e2cdfb9fa0478aa30e3bbbd9bf2563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98cbb978eca8434b839db8eaa184ac0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26a78c5dd15046b88094e981357200c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "387c2ecbbf12418ab12e069aa7a810b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e111b0aac14b47eb9a8a8f7736ae761b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b237b143be24291b4c1dda80e593534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (6) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d860284c0b44df9f993c0b181fe054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "427cb48be9534dcb80ce6a33a071d6c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b511534aa3e94c0d9786d6a1ec9869de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd71dfa4c6a3461388aea9dfff0bde2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cf85cc83911433cb90698a9b25e0e42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d214b29ee36457a98ca259845ad734e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a3cd061db8d4d51a8dae8a24eb6dc8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9110e1c0c2c24442a8d7c8e317659686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b9045e21fec44089f2dfef85ce67d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f865cdef1f0d46d480fb63faa03803ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e8bc06c50344cefab70790e297e8b19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6948a9d51e324f8197e240ba5e8cac07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0c3d51576642d4902d27e4f03fca3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebd3eca4e9e04f9487f254480d94501e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0e0aef4e74469eb130479927729d55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e286473c8c944e6b0a12e4431215caa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (7) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de6018dc79964449b4e0ad303b47c9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13266eeb8e2e48a99a61bf4fbbf5653e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a04b003e54e42aaaa813830c9622daa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c560f1d40e54662be2db9fd5af6e2c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "  | Name          | Type             | Params\n",
      "---------------------------------------------------\n",
      "0 | criterion     | MSELoss          | 0     \n",
      "1 | train_metrics | MetricCollection | 0     \n",
      "2 | val_metrics   | MetricCollection | 0     \n",
      "3 | rnn           | LSTM             | 1.8 K \n",
      "4 | V             | Linear           | 21    \n",
      "---------------------------------------------------\n",
      "1.9 K     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 K     Total params\n",
      "0.007     Total estimated model params size (MB)\n",
      "/Users/jakubkosmydel/miniconda3/envs/rekomendacyjne/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (8) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c0298b9d134a3cbfe4193df0adeba4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mape': 278.1219967679279,\n",
       " 'mse': 0.9045972155781821,\n",
       " 'rmse': 0.9511031571697058,\n",
       " 'mae': 0.7335105641459334,\n",
       " 'model': 'RNNModel',\n",
       " 'forecast_horizon': 3,\n",
       " 'dataset': 'walmart-sales',\n",
       " 'experiment_time': 1715279787.3293781,\n",
       " 'parameters': OrderedDict([('model', 'LSTM'),\n",
       "              ('hidden_dim', 20),\n",
       "              ('n_rnn_layers', 1),\n",
       "              ('dropout', 0.1),\n",
       "              ('training_length', 24),\n",
       "              ('batch_size', 16),\n",
       "              ('n_epochs', 100),\n",
       "              ('optimizer_kwargs', {'lr': 0.001}),\n",
       "              ('model_name', 'LSTM'),\n",
       "              ('log_tensorboard', True),\n",
       "              ('input_chunk_length', 1),\n",
       "              ('random_state', 42),\n",
       "              ('pl_trainer_kwargs', {'accelerator': 'cpu'})])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from darts.models.forecasting.rnn_model import RNNModel\n",
    "\n",
    "lstm_model = RNNModel(\n",
    "    model='LSTM',\n",
    "    hidden_dim=20,\n",
    "    dropout=0.1,\n",
    "    batch_size=16,\n",
    "    n_epochs=100,\n",
    "    optimizer_kwargs={'lr': 1e-3},\n",
    "    model_name='LSTM',\n",
    "    input_chunk_length=1,\n",
    "    random_state=42,\n",
    "    pl_trainer_kwargs={'accelerator': 'cpu'}\n",
    ")\n",
    "\n",
    "experiment_lstm = TimeseriesExperiment(lstm_model, dataset, retrain=True)\n",
    "\n",
    "experiment_lstm.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rekomendacyjne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
